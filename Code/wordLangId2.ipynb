{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "fa94fa03635b191b62c3bdd9f3230a84a78e4138ecf162410bfe25b45739a4b0"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read English Training Data\r\n",
    "# By: Benjamin Kulis\r\n",
    "import string, math\r\n",
    "\r\n",
    "input_file = open('../Data/Input/LangId.train.English', 'r', encoding = 'Latin1')\r\n",
    "lines = input_file.readlines()\r\n",
    "input_file.close()\r\n",
    "\r\n",
    "words = []\r\n",
    "english_vocab = []\r\n",
    "\r\n",
    "for l in lines:\r\n",
    "    for p in string.punctuation:\r\n",
    "        l = l.replace(p, \"\")\r\n",
    "    words.append(l.strip().lower())\r\n",
    "    \r\n",
    "i = 0\r\n",
    "while i < len(words):\r\n",
    "    english_vocab.extend(words[i].split())\r\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create English Bigram\n",
    "english_currWords = {}\n",
    "english_prevWord = {}\n",
    "english_bigram = {}\n",
    "\n",
    "for i in range(len(english_vocab) - 1):\n",
    "    if (english_vocab[i], english_vocab[i + 1]) in english_currWords.keys():\n",
    "        english_currWords[(english_vocab[i], english_vocab[i + 1])] += 1\n",
    "    else:\n",
    "        english_currWords[(english_vocab[i], english_vocab[i + 1])] = 1\n",
    "for i in range(len(english_vocab)):\n",
    "    if english_vocab[i] in english_prevWord.keys():\n",
    "        english_prevWord[english_vocab[i]] += 1\n",
    "    else:\n",
    "        english_prevWord[english_vocab[i]] = 1\n",
    "        \n",
    "for key in english_currWords.keys():\n",
    "    english_bigram[(key, key[0])] = english_currWords[key] / english_prevWord[key[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read French Training Data\n",
    "input_file = open('../Data/Input/LangId.train.French', 'r', encoding = 'Latin1')\n",
    "lines = input_file.readlines()\n",
    "input_file.close()\n",
    "\n",
    "words = []\n",
    "french_vocab = []\n",
    "\n",
    "for l in lines:\n",
    "    for p in string.punctuation:\n",
    "        l = l.replace(p, \"\")\n",
    "    words.append(l.strip().lower())\n",
    "    \n",
    "i = 0\n",
    "while i < len(words):\n",
    "    french_vocab.extend(words[i].split())\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create French Bigram\n",
    "french_currWords = {}\n",
    "french_prevWord = {}\n",
    "french_bigram = {}\n",
    "\n",
    "for i in range(len(french_vocab) - 1):\n",
    "    if (french_vocab[i], french_vocab[i + 1]) in french_currWords.keys():\n",
    "        french_currWords[(french_vocab[i], french_vocab[i + 1])] += 1\n",
    "    else:\n",
    "        french_currWords[(french_vocab[i], french_vocab[i + 1])] = 1\n",
    "for i in range(len(french_vocab)):\n",
    "    if french_vocab[i] in french_prevWord.keys():\n",
    "        french_prevWord[french_vocab[i]] += 1\n",
    "    else:\n",
    "        french_prevWord[french_vocab[i]] = 1\n",
    "        \n",
    "for key in french_currWords.keys():\n",
    "    french_bigram[(key, key[0])] = french_currWords[key] / french_prevWord[key[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Italian Training Data\n",
    "input_file = open('../Data/Input/LangId.train.Italian', 'r', encoding = 'Latin1')\n",
    "lines = input_file.readlines()\n",
    "input_file.close()\n",
    "\n",
    "words = []\n",
    "italian_vocab = []\n",
    "\n",
    "for l in lines:\n",
    "    for p in string.punctuation:\n",
    "        l = l.replace(p, \"\")\n",
    "    words.append(l.strip().lower())\n",
    "    \n",
    "i = 0\n",
    "while i < len(words):\n",
    "    italian_vocab.extend(words[i].split())\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Italian Bigram\n",
    "italian_currWords = {}\n",
    "italian_prevWord = {}\n",
    "italian_bigram = {}\n",
    "\n",
    "for i in range(len(italian_vocab) - 1):\n",
    "    if (italian_vocab[i], italian_vocab[i + 1]) in italian_currWords.keys():\n",
    "        italian_currWords[(italian_vocab[i], italian_vocab[i + 1])] += 1\n",
    "    else:\n",
    "        italian_currWords[(italian_vocab[i], italian_vocab[i + 1])] = 1\n",
    "for i in range(len(italian_vocab)):\n",
    "    if italian_vocab[i] in italian_prevWord.keys():\n",
    "        italian_prevWord[italian_vocab[i]] += 1\n",
    "    else:\n",
    "        italian_prevWord[italian_vocab[i]] = 1\n",
    "        \n",
    "for key in italian_currWords.keys():\n",
    "    italian_bigram[(key, key[0])] = italian_currWords[key] / italian_prevWord[key[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Vocabulary for Test Data\n",
    "test_file = open('../Data/Validation/LangId.test', 'r', encoding = 'Latin1')\n",
    "lines = test_file.readlines()\n",
    "test_file.close()\n",
    "\n",
    "test_vocab = []\n",
    "for l in lines:\n",
    "    words = []\n",
    "    line_vocab = []\n",
    "    for p in string.punctuation:\n",
    "        l = l.replace(p, \"\")\n",
    "    words.append(l.strip().lower())\n",
    "\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        line_vocab.extend(words[i].split())\n",
    "        i += 1\n",
    "    test_vocab.append(line_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test English\n",
    "english_test_log = []\n",
    "\n",
    "for line_vocab in test_vocab:\n",
    "    test_log = 0\n",
    "    test_currWords = []\n",
    "    test_prevWord = []\n",
    "    for i in range(len(line_vocab) - 1):\n",
    "        test_currWords.append((line_vocab[i], line_vocab[i + 1]))\n",
    "    for word in line_vocab:\n",
    "        test_prevWord.append(word)\n",
    "\n",
    "    # Apply GT smoothing\n",
    "    # Sets up the bigram and pairs of words and adds the ones that havent been seen\n",
    "    for word in test_currWords:\n",
    "        if word[0] not in english_prevWord.keys():\n",
    "            english_prevWord[word[0]] = 0\n",
    "        if word not in english_currWords.keys():\n",
    "            english_currWords[word] = 0\n",
    "        if (word, word[0]) not in english_bigram.keys():\n",
    "            english_bigram[(word, word[0])] = 0\n",
    "\n",
    "    total_distinct_words = len(english_prevWord.keys())\n",
    "    possible_word_pairs = total_distinct_words * total_distinct_words\n",
    "    seen_distinct_pairs = len(english_currWords.keys())\n",
    "\n",
    "    count_appearances = {}\n",
    "    for key in english_currWords.keys():\n",
    "        if english_currWords[key] in count_appearances.keys():\n",
    "            count_appearances[english_currWords[key]] += 1\n",
    "        else:\n",
    "            count_appearances[english_currWords[key]] = 1\n",
    "    count_appearances[0] = (possible_word_pairs - seen_distinct_pairs)\n",
    "\n",
    "    sorted_appearances = sorted(count_appearances.keys())\n",
    "\n",
    "    total_seen_pairs = 0\n",
    "    for c in count_appearances.keys():\n",
    "        total_seen_pairs += (c * count_appearances[c])\n",
    "\n",
    "    # Calculate C star\n",
    "    count = 0\n",
    "    cStarDict = {}\n",
    "    for c in range(len(sorted_appearances)):\n",
    "        if count >= len(sorted_appearances) - 1:\n",
    "            cStarDict[sorted_appearances[c]] = cStarDict[0]\n",
    "        if count < len(sorted_appearances) - 1:\n",
    "            cStarDict[sorted_appearances[c]] = (sorted_appearances[c + 1] * count_appearances[sorted_appearances[c + 1]]) / count_appearances[sorted_appearances[c]]\n",
    "        count += 1\n",
    "\n",
    "    # Calculate P star\n",
    "    pStarDict = {}\n",
    "    for c in cStarDict.keys():\n",
    "        pStarDict[c] = (cStarDict[c] * count_appearances[c]) / total_seen_pairs\n",
    "\n",
    "    # Update bigram probabilities\n",
    "    for i in english_bigram:\n",
    "        c = english_currWords[i[0]]\n",
    "        english_bigram[i] = pStarDict[c] / count_appearances[c]\n",
    "\n",
    "    for word in range(len(line_vocab) - 1):\n",
    "        test_log += math.log(english_bigram[((line_vocab[word], line_vocab[word + 1]), (line_vocab[word], line_vocab[word + 1])[0])], 2)\n",
    "    english_test_log.append(test_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test French\n",
    "french_test_log = []\n",
    "\n",
    "for line_vocab in test_vocab:\n",
    "    test_log = 0\n",
    "    test_currWords = []\n",
    "    test_prevWord = []\n",
    "    for i in range(len(line_vocab) - 1):\n",
    "        test_currWords.append((line_vocab[i], line_vocab[i + 1]))\n",
    "    for word in line_vocab:\n",
    "        test_prevWord.append(word)\n",
    "\n",
    "    # Apply GT smoothing\n",
    "    # Sets up the bigram and pairs of words and adds the ones that havent been seen\n",
    "    for word in test_currWords:\n",
    "        if word[0] not in french_prevWord.keys():\n",
    "            french_prevWord[word[0]] = 0\n",
    "        if word not in french_currWords.keys():\n",
    "            french_currWords[word] = 0\n",
    "        if (word, word[0]) not in french_bigram.keys():\n",
    "            french_bigram[(word, word[0])] = 0\n",
    "\n",
    "    total_distinct_words = len(french_prevWord.keys())\n",
    "    possible_word_pairs = total_distinct_words * total_distinct_words\n",
    "    seen_distinct_pairs = len(french_currWords.keys())\n",
    "\n",
    "    count_appearances = {}\n",
    "    for key in french_currWords.keys():\n",
    "        if french_currWords[key] in count_appearances.keys():\n",
    "            count_appearances[french_currWords[key]] += 1\n",
    "        else:\n",
    "            count_appearances[french_currWords[key]] = 1\n",
    "    count_appearances[0] = (possible_word_pairs - seen_distinct_pairs)\n",
    "\n",
    "    sorted_appearances = sorted(count_appearances.keys())\n",
    "\n",
    "    total_seen_pairs = 0\n",
    "    for c in count_appearances.keys():\n",
    "        total_seen_pairs += (c * count_appearances[c])\n",
    "\n",
    "    # Calculate C star\n",
    "    count = 0\n",
    "    cStarDict = {}\n",
    "    for c in range(len(sorted_appearances)):\n",
    "        if count >= len(sorted_appearances) - 1:\n",
    "            cStarDict[sorted_appearances[c]] = cStarDict[0]\n",
    "        if count < len(sorted_appearances) - 1:\n",
    "            cStarDict[sorted_appearances[c]] = (sorted_appearances[c + 1] * count_appearances[sorted_appearances[c + 1]]) / count_appearances[sorted_appearances[c]]\n",
    "        count += 1\n",
    "\n",
    "    # Calculate P star\n",
    "    pStarDict = {}\n",
    "    for c in cStarDict.keys():\n",
    "        pStarDict[c] = (cStarDict[c] * count_appearances[c]) / total_seen_pairs\n",
    "\n",
    "    # Update bigram probabilities\n",
    "    for i in french_bigram:\n",
    "        c = french_currWords[i[0]]\n",
    "        french_bigram[i] = pStarDict[c] / count_appearances[c]\n",
    "\n",
    "    for word in range(len(line_vocab) - 1):\n",
    "        test_log += math.log(french_bigram[((line_vocab[word], line_vocab[word + 1]), (line_vocab[word], line_vocab[word + 1])[0])], 2)\n",
    "    french_test_log.append(test_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Italian\n",
    "italian_test_log = []\n",
    "\n",
    "for line_vocab in test_vocab:\n",
    "    test_log = 0\n",
    "    test_currWords = []\n",
    "    test_prevWord = []\n",
    "    for i in range(len(line_vocab) - 1):\n",
    "        test_currWords.append((line_vocab[i], line_vocab[i + 1]))\n",
    "    for word in line_vocab:\n",
    "        test_prevWord.append(word)\n",
    "\n",
    "    # Apply GT smoothing\n",
    "    # Sets up the bigram and pairs of words and adds the ones that havent been seen\n",
    "    for word in test_currWords:\n",
    "        if word[0] not in italian_prevWord.keys():\n",
    "            italian_prevWord[word[0]] = 0\n",
    "        if word not in italian_currWords.keys():\n",
    "            italian_currWords[word] = 0\n",
    "        if (word, word[0]) not in italian_bigram.keys():\n",
    "            italian_bigram[(word, word[0])] = 0\n",
    "\n",
    "    total_distinct_words = len(italian_prevWord.keys())\n",
    "    possible_word_pairs = total_distinct_words * total_distinct_words\n",
    "    seen_distinct_pairs = len(italian_currWords.keys())\n",
    "\n",
    "    count_appearances = {}\n",
    "    for key in italian_currWords.keys():\n",
    "        if italian_currWords[key] in count_appearances.keys():\n",
    "            count_appearances[italian_currWords[key]] += 1\n",
    "        else:\n",
    "            count_appearances[italian_currWords[key]] = 1\n",
    "    count_appearances[0] = (possible_word_pairs - seen_distinct_pairs)\n",
    "\n",
    "    sorted_appearances = sorted(count_appearances.keys())\n",
    "\n",
    "    total_seen_pairs = 0\n",
    "    for c in count_appearances.keys():\n",
    "        total_seen_pairs += (c * count_appearances[c])\n",
    "\n",
    "    # Calculate C star\n",
    "    count = 0\n",
    "    cStarDict = {}\n",
    "    for c in range(len(sorted_appearances)):\n",
    "        if count >= len(sorted_appearances) - 1:\n",
    "            cStarDict[sorted_appearances[c]] = cStarDict[0]\n",
    "        if count < len(sorted_appearances) - 1:\n",
    "            cStarDict[sorted_appearances[c]] = (sorted_appearances[c + 1] * count_appearances[sorted_appearances[c + 1]]) / count_appearances[sorted_appearances[c]]\n",
    "        count += 1\n",
    "\n",
    "    # Calculate P star\n",
    "    pStarDict = {}\n",
    "    for c in cStarDict.keys():\n",
    "        pStarDict[c] = (cStarDict[c] * count_appearances[c]) / total_seen_pairs\n",
    "\n",
    "    # Update bigram probabilities\n",
    "    for i in italian_bigram:\n",
    "        c = italian_currWords[i[0]]\n",
    "        italian_bigram[i] = pStarDict[c] / count_appearances[c]\n",
    "\n",
    "    for word in range(len(line_vocab) - 1):\n",
    "        test_log += math.log(italian_bigram[((line_vocab[word], line_vocab[word + 1]), (line_vocab[word], line_vocab[word + 1])[0])], 2)\n",
    "    italian_test_log.append(test_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare and Write Predictions to Output File\n",
    "output_file = open('../Data/Output/wordLangId2.out', 'w')\n",
    "\n",
    "for i in range(len(english_test_log)):\n",
    "    lang_prediction = max(english_test_log[i], french_test_log[i], italian_test_log[i])\n",
    "\n",
    "    if lang_prediction == english_test_log[i]:\n",
    "        output_file.write(str(i + 1) + \" English\")\n",
    "    elif lang_prediction == french_test_log[i]:\n",
    "        output_file.write(str(i + 1) + \" French\")\n",
    "    else:\n",
    "        output_file.write(str(i + 1) + \" Italian\")\n",
    "\n",
    "    if (i + 1 < len(english_test_log)):\n",
    "         output_file.write(\"\\n\")\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Prediction Accuracy: 99.66666666666667%\n"
     ]
    }
   ],
   "source": [
    "# Check Prediction Accuracy\n",
    "prediction_file = open('../Data/Output/wordLangId2.out', 'r')\n",
    "prediction_lines = prediction_file.readlines()\n",
    "prediction_file.close()\n",
    "\n",
    "solutions_file = open('../Data/Validation/labels.sol', 'r')\n",
    "solution_lines = solutions_file.readlines()\n",
    "solutions_file.close()\n",
    "\n",
    "correct = 0\n",
    "for i in range(len(solution_lines)):\n",
    "    if prediction_lines[i] == solution_lines[i]:\n",
    "        correct += 1\n",
    "print(\"Prediction Accuracy: \" + str((correct/len(solution_lines)) * 100) + \"%\")"
   ]
  }
 ]
}